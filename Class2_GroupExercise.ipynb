{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Class 2 Group Exercise: Building a Diabetes Risk Prediction Model\n",
    "\n",
    "## Background\n",
    "The Pima Indians Diabetes dataset contains diagnostic measurements from female patients of Pima Indian heritage. Your task is to build and evaluate a predictive model for diabetes diagnosis.\n",
    "\n",
    "## Your Task\n",
    "Working in groups, build a complete predictive modeling pipeline from data exploration to model evaluation.\n",
    "\n",
    "This exercise combines skills from:\n",
    "- Understanding distributions (CentralLimitExample)\n",
    "- Bootstrap confidence intervals (bootstrapping)\n",
    "- Logistic regression modeling (LogisticRegressionSignificance)\n",
    "- Model evaluation with ROC/AUC (How-to-Roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "## Part 1: Data Exploration & Understanding Distributions (15 minutes)\n",
    "\n",
    "1. Load the Pima Indians dataset and examine its structure\n",
    "2. Check the distribution of the outcome variable (diabetes: Yes/No)\n",
    "3. For each predictor, create histograms and assess normality\n",
    "4. Identify any predictors that might need transformation\n",
    "5. Check for missing values or suspicious zeros (hint: can BMI = 0?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "\n",
    "# Load data\n",
    "data(Pima.tr, package=\"MASS\")\n",
    "data(Pima.te, package=\"MASS\")\n",
    "\n",
    "# Combine for exploration\n",
    "pima_full <- rbind(Pima.tr, Pima.te)\n",
    "head(pima_full)\n",
    "summary(pima_full)\n",
    "\n",
    "# Your code here: Check outcome distribution\n",
    "table(pima_full$type)\n",
    "\n",
    "# Your code here: Histograms of predictors\n",
    "\n",
    "\n",
    "# Your code here: Check for suspicious zeros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "## Part 2: Train/Test Split & Bootstrap Baseline (10 minutes)\n",
    "\n",
    "1. Split the data into 70% training and 30% test sets\n",
    "2. Calculate the baseline diabetes prevalence in your training set\n",
    "3. Use bootstrapping to estimate the 95% confidence interval for this prevalence\n",
    "4. This baseline tells us: if we predicted \"No diabetes\" for everyone, what would our accuracy be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)  # For reproducibility\n",
    "\n",
    "# Train/test split\n",
    "train_idx <- sample(1:nrow(pima_full), 0.7 * nrow(pima_full))\n",
    "train_data <- pima_full[train_idx, ]\n",
    "test_data <- pima_full[-train_idx, ]\n",
    "\n",
    "cat(\"Training set size:\", nrow(train_data), \"\\n\")\n",
    "cat(\"Test set size:\", nrow(test_data), \"\\n\")\n",
    "\n",
    "# Your code here: Calculate baseline prevalence\n",
    "\n",
    "\n",
    "# Your code here: Bootstrap CI for prevalence\n",
    "# Hint: Create a function that calculates proportion of \"Yes\" and use replicate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "## Part 3: Build Logistic Regression Models (15 minutes)\n",
    "\n",
    "Build and compare multiple logistic regression models:\n",
    "\n",
    "1. **Model 1 (Simple):** Predict diabetes using only `glu` (glucose)\n",
    "2. **Model 2 (Clinical):** Add `bmi` and `age` to the model\n",
    "3. **Model 3 (Full):** Include all available predictors\n",
    "\n",
    "For each model:\n",
    "- Examine the summary and identify significant predictors\n",
    "- Note the AIC value for model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Simple - glucose only\n",
    "model1 <- glm(type ~ glu, data = train_data, family = binomial)\n",
    "summary(model1)\n",
    "\n",
    "# Your code here: Model 2 - glucose, BMI, age\n",
    "\n",
    "\n",
    "# Your code here: Model 3 - all predictors\n",
    "# Predictors available: npreg, glu, bp, skin, bmi, ped, age\n",
    "\n",
    "\n",
    "# Compare AIC values\n",
    "cat(\"Model 1 AIC:\", AIC(model1), \"\\n\")\n",
    "# Add AIC for other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "## Part 4: Model Evaluation - ROC Curves & AUC (15 minutes)\n",
    "\n",
    "Evaluate your models on the **test set** (not training set!):\n",
    "\n",
    "1. Generate predicted probabilities for each model on the test set\n",
    "2. Create ROC curves for all three models\n",
    "3. Calculate AUC for each model\n",
    "4. Which model performs best on unseen data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(pROC)\n",
    "\n",
    "# Predictions on test set - Model 1\n",
    "pred1 <- predict(model1, newdata = test_data, type = \"response\")\n",
    "\n",
    "# ROC curve for Model 1\n",
    "roc1 <- roc(test_data$type, pred1)\n",
    "plot(roc1, main = \"ROC Curves Comparison\", col = \"red\")\n",
    "cat(\"Model 1 AUC:\", auc(roc1), \"\\n\")\n",
    "\n",
    "# Your code here: Predictions and ROC for Model 2\n",
    "\n",
    "\n",
    "# Your code here: Predictions and ROC for Model 3\n",
    "\n",
    "\n",
    "# Add all ROC curves to the same plot for comparison\n",
    "# Use: plot(roc2, add=TRUE, col=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-header",
   "metadata": {},
   "source": [
    "## Part 5: Threshold Selection & Confusion Matrix (10 minutes)\n",
    "\n",
    "For your best model:\n",
    "\n",
    "1. Using a threshold of 0.5, create predictions and a confusion matrix\n",
    "2. Calculate sensitivity (true positive rate) and specificity (true negative rate)\n",
    "3. In a clinical screening context, would you prefer higher sensitivity or specificity? Why?\n",
    "4. Try a threshold of 0.3 - how does this change sensitivity and specificity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your best model's predictions\n",
    "best_pred <- pred1  # Change this to your best model's predictions\n",
    "\n",
    "# Threshold = 0.5\n",
    "pred_class_50 <- ifelse(best_pred > 0.5, \"Yes\", \"No\")\n",
    "table(Predicted = pred_class_50, Actual = test_data$type)\n",
    "\n",
    "# Your code here: Calculate sensitivity and specificity for threshold = 0.5\n",
    "# Sensitivity = TP / (TP + FN)\n",
    "# Specificity = TN / (TN + FP)\n",
    "\n",
    "\n",
    "# Your code here: Try threshold = 0.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-header",
   "metadata": {},
   "source": [
    "## Part 6: Bootstrap Model Performance (Optional Challenge - 10 minutes)\n",
    "\n",
    "Use bootstrapping to estimate the uncertainty in your model's AUC:\n",
    "\n",
    "1. Bootstrap the test set 1000 times\n",
    "2. For each bootstrap sample, calculate the AUC\n",
    "3. Report the 95% confidence interval for the AUC\n",
    "4. Is the AUC significantly better than 0.5 (random guessing)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part6-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap AUC estimation\n",
    "set.seed(456)\n",
    "\n",
    "bootstrap_auc <- function(data, predictions) {\n",
    "  idx <- sample(1:nrow(data), replace = TRUE)\n",
    "  boot_data <- data[idx, ]\n",
    "  boot_pred <- predictions[idx]\n",
    "  roc_obj <- roc(boot_data$type, boot_pred, quiet = TRUE)\n",
    "  return(as.numeric(auc(roc_obj)))\n",
    "}\n",
    "\n",
    "# Your code here: Run 1000 bootstrap iterations\n",
    "# auc_bootstrap <- replicate(1000, bootstrap_auc(test_data, best_pred))\n",
    "\n",
    "\n",
    "# Your code here: Calculate 95% CI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discussion",
   "metadata": {},
   "source": [
    "## Group Discussion Questions\n",
    "\n",
    "1. **Model Selection:** Which model would you recommend for clinical use? Consider:\n",
    "   - Predictive performance (AUC)\n",
    "   - Interpretability\n",
    "   - Practical data availability\n",
    "\n",
    "2. **Clinical Utility:** If this model were used for diabetes screening:\n",
    "   - What threshold would you recommend?\n",
    "   - What are the consequences of false positives vs false negatives?\n",
    "   - How would you explain the model output to a patient?\n",
    "\n",
    "3. **Limitations:** What are the limitations of this analysis?\n",
    "   - Sample characteristics (Pima Indian women only)\n",
    "   - Missing data handling\n",
    "   - Temporal validation\n",
    "\n",
    "4. **Next Steps:** What would you do to improve this model?\n",
    "   - Feature engineering\n",
    "   - Different algorithms\n",
    "   - External validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "notes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your group's notes and conclusions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
